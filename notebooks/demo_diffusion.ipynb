{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démo DDPM\n",
    "\n",
    "## Definition\n",
    "\n",
    "Les diffusions modèles (ici [DDPM](https://arxiv.org/abs/2006.11239)) sont une branche d'algorithmes utilisés pour la génération de données.\n",
    "\n",
    "![](https://learnopencv.com/wp-content/uploads/2023/01/diffusion-models-forwardbackward_process_ddpm.png)\n",
    "\n",
    "Le principe est simple, il y a un premier processus de diffusion consistant à ajouter progressivement du bruit à la donnée. Ce processus de diffusion consiste en une chaine de Markov où il est possible d'obtenir la donnée à t + 1 (donc un peu plus bruitée) à partir de la donnée à un temps t :\n",
    "\n",
    "![](./resources/diffusion_formula.png)\n",
    "\n",
    "où les betas représentent la variance à un temps t, et sont croissants linéairement sur 1000 étapes de 1e-4 (beta 0) à 2e-2 (beta 1000).\n",
    "\n",
    "De l'autre côté, nous avons une seconde chaine de Markov qui vise à de-bruiter l'image précédemment diffusée (bruitée) :\n",
    "\n",
    "![](./resources/reverse_formula.png)\n",
    "\n",
    "Regardons ça de plus près, mais d'abord des imports !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Literal\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward process - diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://media.cnn.com/api/v1/images/stellar/prod/190430171751-mona-lisa.jpg\n",
    "\n",
    "x_0 = to_tensor(Image.open(\"./190430171751-mona-lisa.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "beta_1 = 1e-3\n",
    "beta_t = 2e-1\n",
    "\n",
    "betas = th.linspace(beta_1, beta_t, steps=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_step(x_t_prev: th.Tensor, t: int) -> th.Tensor:\n",
    "    z = th.randn_like(x_t_prev)\n",
    "    return (1 - betas[t]) * x_t_prev + z * betas[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = q_step(x_0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0.size(), x_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_0.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_1.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_list = [x_0]\n",
    "\n",
    "for t in tqdm(range(1, T)):\n",
    "    x_t_list.append(q_step(x_t_list[-1], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_t_list[25].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_t_list[40].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_t_list[-1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 secondes pour \"diffuser\" une image, c'est trop ! Les auteurs proposent une simplification permettant d'obtenir n'import quel x (de t = 1 à t = T) à partir de la donnée originelle, le x à t = 0 :\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:660/1*SRUnVsytTzuCWLvu7tA4gA.png)\n",
    "\n",
    "Ce qui donne en code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 1 - betas\n",
    "alphas_cum_prod = th.cumprod(alphas, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x_0: th.Tensor, t: int) -> th.Tensor:\n",
    "    z = th.randn_like(x_0)\n",
    "    return th.sqrt(alphas_cum_prod[t]) * x_0 + (1 - alphas_cum_prod[t]) * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_10 = q_sample(x_0, 10)\n",
    "plt.matshow(x_10.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_30 = q_sample(x_0, 30)\n",
    "plt.matshow(x_30.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_70 = q_sample(x_0, 70)\n",
    "plt.matshow(x_70.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse process - denoising process\n",
    "\n",
    "C'est ok pour le processus de diffusion, qu'en est-il pour le coeur du sujet : le processus inverse aka le de-bruitage ?\n",
    "\n",
    "Il s'agit aussi d'une chaine de markov : à une étape t, il faut prédire la moyenne et la matrice de covariance qui ont servi à ajouter le bruit à l'étape précédente (pour passer de t - 1 à t) :\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRCZCpFnGkY_TCOPvvy-W3rOBuKR-ZPTbx6Pg&usqp=CAU)\n",
    "\n",
    "Le modèle reçoit deux paramètres :\n",
    "- la donnée bruitée\n",
    "- l'indice dans la chaine de markov\n",
    "\n",
    "Les auteurs simplifient le tout (en rapport au processus de diffusion / bruitage amélioré) :\n",
    "- on oublie la matrice de covariance du fait que la variance sera constante (matrice identité avec comme facteur beta)\n",
    "- on ne prédit plus la moyenne de la distribution normale, mais directement le bruit\n",
    "\n",
    "Le tout donne en algorithme :\n",
    "\n",
    "![](https://huggingface.co/blog/assets/78_annotated-diffusion/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net architecture\n",
    "\n",
    "Il nous faut une architecture de réseau de neurones qui puisse à partir d'une image, produire une image de mêmes dimensions mais dans un espace de canaux / pixels / couleurs différent. Ici l'espace à prédire pour les pixels sera le bruit qui a été ajouté.\n",
    "\n",
    "L'architecture U-Net sera parfaite : elle a fait ses preuves dans le biomédical pour de la segmentation d'images (plus généralement passer dans un autre espace de couleurs / canaux / pixels). Elle consiste en deux parties :\n",
    "- un encodeur\n",
    "- un décodeur\n",
    "\n",
    "Ces deux parties sont liées par le \"milieu\" (la connexion encodeur vers décodeur) mais aussi par des connexions directes entre les couches de l'encodeur et celles du décodeur :\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*f7YOaE4TWubwaFF7Z1fzNw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'élément de base : la convolution\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif?20190413174630)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "Nous allons créer notre block (ou couche) de base comprenant :\n",
    "- une convolution 3 x 3\n",
    "- une activation : Mish\n",
    "- une couche de normalisation : GroupNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        group_norm: int,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=(1, 1),\n",
    "                stride=(1, 1),\n",
    "            ),\n",
    "            nn.Mish(),\n",
    "            nn.GroupNorm(group_norm, out_channels),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = ConvBlock(3, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_30.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_30 = x_30.unsqueeze(0)\n",
    "out = block_1(x_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up sample / down sample\n",
    "\n",
    "Pour diminuer la taille de nos images latentes (celles entre les couches de l'encodeur) : on applique un pas de 2 pour nos convolutions.\n",
    "\n",
    "Pour augmenter la taille des images latentes (celles entre les couches du décodeur) : des convolutions transposées à pas de 2.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/1*kOThnLR8Fge_AJcHrkR3dg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrideConvBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        norm_groups: int,\n",
    "        mode: Literal[\"up\", \"down\"],\n",
    "    ) -> None:\n",
    "        conv_constructors = {\n",
    "            \"up\": nn.ConvTranspose2d,\n",
    "            \"down\": nn.Conv2d,\n",
    "        }\n",
    "\n",
    "        super().__init__(\n",
    "            conv_constructors[mode](\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(4, 4),\n",
    "                padding=(1, 1),\n",
    "                stride=(2, 2)\n",
    "            ),\n",
    "            nn.Mish(),\n",
    "            nn.GroupNorm(norm_groups, out_channels),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block = StrideConvBlock(8, 16, 4, \"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2 = down_block(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size(), out_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_block = StrideConvBlock(16, 8, 4, \"up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_3 = up_block(out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2.size(), out_3.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time embedding\n",
    "\n",
    "Nous disposons maintenant des briques de base pour notre U-Net. Il ne manque plus qu'à intégrer le paramètre supplémentaire représentant l'indice de l'étape dans le processus de diffusion :\n",
    "$$\\mu_{\\theta }(x_{t},t)$$\n",
    "\n",
    "L'idée : \"injecter\" à chaque couche (ie chaque image intermédiaire / image latente) l'information du temps. Soucis : d'un coté (image) on a des valeurs continues, de l'autr coté (indice de l'étape) des valeurs discrètes.\n",
    "La solution : des vecteurs d'embedding.\n",
    "\n",
    "Les auteurs de DDPM utilisent un embedding sans paramètres (sans avoir besoin d'être entrainé) pour représenter un indice d'étape dans la chaine de markov : l'embedding sinusoïde aka le positional embedding (cf. Attention is all you need).\n",
    "\n",
    "![](https://i.stack.imgur.com/67ADh.png)\n",
    "\n",
    "Ce qui donne en code :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_EMBEDDING_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = th.zeros(T, TIME_EMBEDDING_SIZE)\n",
    "print(pos_emb.size())\n",
    "\n",
    "position = th.arange(0, T).unsqueeze(1)\n",
    "print(position.size())\n",
    "\n",
    "div_term = th.exp(\n",
    "    th.arange(0, TIME_EMBEDDING_SIZE, 2, dtype=th.float)\n",
    "    * th.tensor(-math.log(10000.0) / T)\n",
    ")\n",
    "print(div_term.size())\n",
    "\n",
    "pos_emb[:, 0::2] = th.sin(position.float() * div_term)\n",
    "pos_emb[:, 1::2] = th.cos(position.float() * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok mais maintenant comment on l'injecte ? D'un côté j'ai une matrice carrée (même cubique si on compte les \"couleurs\" latentes) et de l'autre côté un seul vecteur ?\n",
    "\n",
    "1. Projeter (avec des paramètres entrainables !) le vecteur d'embedding de temps dans l'espace des \"couleurs\" de l'image latente / intermédiaire\n",
    "2. Ajouter ce vecteur à chaque pixel de l'image\n",
    "\n",
    "En code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_proj = nn.Sequential(\n",
    "    nn.Linear(TIME_EMBEDDING_SIZE, 8),\n",
    "    nn.Mish(),\n",
    "    nn.Linear(8, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first block :\")\n",
    "print(block_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_2 = ConvBlock(8, 16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(block_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_30.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = th.randint(0, T, (x_30.size(0),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding = pos_emb[t]\n",
    "print(\"emb\", t_embedding.size())\n",
    "\n",
    "t_projected = time_proj(t_embedding)\n",
    "print(\"proj\", t_projected.size())\n",
    "\n",
    "t_projected_unsqueezed = t_projected[:, :, None, None]\n",
    "print(\"proj-unsqueez\", t_projected_unsqueezed.size())\n",
    "\n",
    "out_1 = block_1(x_30)\n",
    "print(\"out-1\", out_1.size())\n",
    "\n",
    "out_2 = out_1 + t_projected_unsqueezed\n",
    "print(\"out-2\", out_2.size())\n",
    "\n",
    "out_3 = block_2(out_2)\n",
    "print(\"out-3\", out_3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a maintenant (presque) toutes les briques de base du U-Net, voici l'enchainement pour la partie encodeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder's layers definition\n",
    "\n",
    "channels = [(8, 16), (16, 32), (32, 64)]\n",
    "input_channels = 3\n",
    "norm_groups = 4\n",
    "\n",
    "encoder_time_to_channel_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(TIME_EMBEDDING_SIZE, c_i),\n",
    "        nn.Mish(),\n",
    "        nn.Linear(c_i, c_i)\n",
    "    )\n",
    "    for c_i, _ in channels\n",
    ")\n",
    "\n",
    "first_conv = ConvBlock(input_channels, channels[0][0], norm_groups)\n",
    "\n",
    "encoder_conv_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        ConvBlock(c_i, c_o, norm_groups),\n",
    "        ConvBlock(c_o, c_o, norm_groups),\n",
    "    )\n",
    "    for c_i, c_o in channels\n",
    ")\n",
    "\n",
    "down_conv_blocks = nn.ModuleList(\n",
    "    StrideConvBlock(c_o, c_o, norm_groups, \"down\")\n",
    "    for _, c_o in channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs definition\n",
    "\n",
    "x = x_30.clone()\n",
    "t = th.randint(0, T, (x_30.size(0),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder forward\n",
    "\n",
    "out = first_conv(x_30)\n",
    "print(\"out-0\", out.size())\n",
    "\n",
    "time_emb = pos_emb[t]\n",
    "\n",
    "for i, (block, down, time_block) in enumerate(zip(encoder_conv_blocks, down_conv_blocks, encoder_time_to_channel_blocks)):\n",
    "    print(\"layer\", i)\n",
    "    time = time_block(time_emb)[:, :, None, None]\n",
    "    print(\"time\", time.size())\n",
    "    out = out + time\n",
    "    print(\"time-add\", out.size())\n",
    "\n",
    "    out = block(out)\n",
    "    print(\"conv-out\", out.size())\n",
    "\n",
    "    out = down(out)\n",
    "    print(\"strided-conv-out\", out.size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même logique pour le décodeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder's layers definition\n",
    "\n",
    "decoder_channels = [(c_o, c_i) for c_i, c_o in reversed(channels)]\n",
    "\n",
    "up_conv_blocks = nn.ModuleList(\n",
    "    StrideConvBlock(c_i, c_i, norm_groups, \"up\")\n",
    "    for c_i, _ in decoder_channels\n",
    ")\n",
    "\n",
    "decoder_time_to_channels_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(TIME_EMBEDDING_SIZE, c_i),\n",
    "        nn.Mish(),\n",
    "        nn.Linear(c_i, c_i)\n",
    "    )\n",
    "    for c_i, _ in decoder_channels\n",
    ")\n",
    "\n",
    "decoder_conv_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        ConvBlock(c_i, c_i, norm_groups),\n",
    "        ConvBlock(c_i, c_o, norm_groups),\n",
    "    )\n",
    "    for c_i, c_o in decoder_channels\n",
    ")\n",
    "\n",
    "# on output des variables aléatoire d'une distribution normale\n",
    "# => pas de limites théoriques => pas d'activation\n",
    "last_block = nn.Conv2d(\n",
    "    decoder_channels[-1][1],\n",
    "    input_channels,\n",
    "    kernel_size=(3, 3),\n",
    "    stride=(1, 1),\n",
    "    padding=(1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder inputs\n",
    "\n",
    "out.size(), time_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder forward\n",
    "\n",
    "for i, (up, time_block, block) in enumerate(zip(up_conv_blocks, decoder_time_to_channels_blocks, decoder_conv_blocks)):\n",
    "    print(\"layer\", i)\n",
    "\n",
    "    out = up(out)\n",
    "    print(\"up\", out.size())\n",
    "\n",
    "    time_proj = time_block(time_emb)[:, :, None, None]\n",
    "    print(\"time\", time_proj.size())\n",
    "\n",
    "    out = out + time_proj\n",
    "    print(\"time-add\", out.size())\n",
    "\n",
    "    out = block(out)\n",
    "    print(\"conv\", out.size())\n",
    "    print()\n",
    "\n",
    "eps = last_block(out)\n",
    "print(\"eps\", eps.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dernière étape : les connexions \"bypass\" entre encodeur et décodeur\n",
    "\n",
    "L'idée : récupérer l'image intermédiaire de chaque couche de l'encodeur et la concaténer (au niveau de l'axe des canaux) à l'image intermédiaire homologue coté décodeur.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*VUS2cCaPB45wcHHFp_fQZQ.png)\n",
    "\n",
    "\n",
    "Il nous faut modifier les blocks de convolutions du décodeur pour que ceux-ci prennent deux fois plus de canaux en entrée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_conv_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        ConvBlock(c_i * 2, c_i, norm_groups),\n",
    "        ConvBlock(c_i, c_o, norm_groups),\n",
    "    )\n",
    "    for c_i, c_o in decoder_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et du coup le time embedding to channels aussi :\n",
    "\n",
    "decoder_time_to_channels_blocks = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(TIME_EMBEDDING_SIZE, c_i * 2),\n",
    "        nn.Mish(),\n",
    "        nn.Linear(c_i * 2, c_i * 2)\n",
    "    )\n",
    "    for c_i, _ in decoder_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le U-Net et sa fonction forward finie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = first_conv(x_30)\n",
    "\n",
    "time_emb = pos_emb[t]\n",
    "\n",
    "bypasses = []\n",
    "\n",
    "for i, (block, down, time_block) in enumerate(zip(encoder_conv_blocks, down_conv_blocks, encoder_time_to_channel_blocks)):\n",
    "    time = time_block(time_emb)[:, :, None, None]\n",
    "    out = out + time\n",
    "\n",
    "    out = block(out)\n",
    "\n",
    "    bypasses.append(out)\n",
    "\n",
    "    out = down(out)\n",
    "\n",
    "print(\" \".join(str(bypass.size()) for bypass in bypasses))\n",
    "\n",
    "for i, (up, time_block, block, bypass) in enumerate(zip(up_conv_blocks, decoder_time_to_channels_blocks, decoder_conv_blocks, reversed(bypasses))):\n",
    "    out = up(out)\n",
    "\n",
    "    # dim=1  =>  les canaux des pixels\n",
    "    out = th.cat([out, bypass], dim=1)\n",
    "    print(\"decoder\", i, \"size_1 :\", out.size())\n",
    "\n",
    "    time_proj = time_block(time_emb)[:, :, None, None]\n",
    "    out = out + time_proj\n",
    "\n",
    "    out = block(out)\n",
    "    print(\"decoder\", i, \"size_2 :\",out.size())\n",
    "\n",
    "eps = last_block(out)\n",
    "print(eps.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus qu'à mettre le tout en Module PyTorch et se consacrer à l'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_diffusion.networks import Noiser, Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "\n",
    "noiser = Noiser(T, 1e-4, 2e-2)\n",
    "denoiser = Denoiser(\n",
    "    3, T, 16, 1e-4, 2e-2, [(8, 16), (16, 32), (32, 64)], 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/blog/assets/78_annotated-diffusion/training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_batch_size = 2\n",
    "\n",
    "optim = th.optim.Adam(denoiser.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# une seule image dans le batch\n",
    "x_0 = to_tensor(Image.open(\"./190430171751-mona-lisa.jpg\")).unsqueeze(0)\n",
    "t = th.randint(0, T, (x_0.size(0), time_batch_size))\n",
    "\n",
    "# diffusion / bruitage\n",
    "x_t, eps = noiser(x_0, t)\n",
    "\n",
    "# prediction du bruit\n",
    "eps_theta, _ = denoiser(x_t, t)\n",
    "\n",
    "print(x_t.size(), eps.size(), eps_theta.size())\n",
    "\n",
    "# on cherche à prédire le bruit\n",
    "loss = th.pow(eps - eps_theta, 2.)\n",
    "loss = loss.mean()\n",
    "\n",
    "# backward sur le réseau de neurone + MAJ des poids\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(denoiser.parameters()).grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dernière étape : la génération\n",
    "\n",
    "![](https://eugeneyan.com/assets/ddpm-sampling.jpg)\n",
    "\n",
    "En code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3\n",
    "with th.no_grad():\n",
    "    denoiser.cuda()\n",
    "    denoiser.eval()\n",
    "\n",
    "    # on tire une image aléatoire : input pour générer la donnée\n",
    "    x_t = th.randn(1, input_channels, *x_0.size()[2:], device=\"cuda\")\n",
    "\n",
    "    # les étapes de T - 1 à 0\n",
    "    for t in tqdm(reversed(range(T))):\n",
    "        z = th.randn_like(x_t, device=\"cuda\") if t > 0 else th.zeros_like(x_t, device=\"cuda\")\n",
    "\n",
    "        # predire le bruit\n",
    "        eps_theta, _ = denoiser(\n",
    "            x_t.unsqueeze(1),\n",
    "            th.tensor([[t]], device=\"cuda\"),\n",
    "        )\n",
    "        eps_theta = eps_theta.squeeze(1)\n",
    "\n",
    "        # moyenne\n",
    "        mu = (x_t - eps_theta * (1 - alphas[t]) / th.sqrt(1 - alphas_cum_prod[t])) / th.sqrt(alphas[t])\n",
    "\n",
    "        # variance\n",
    "        var = betas[t]\n",
    "\n",
    "        # création du x à l'étape t - 1\n",
    "        x_t = mu + th.sqrt(var) * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà ! Vous maitrisez maintenant le papa des diffusions modèles !!\n",
    "\n",
    "Petit souci : dans la pratique sur la musique ça ne fonctionnait pas au top, let's go voir ça dans un autre notebook !\n",
    "\n",
    "![](https://wompampsupport.azureedge.net/fetchimage?siteId=7575&v=2&jpgQuality=100&width=700&url=https%3A%2F%2Fi.kym-cdn.com%2Fentries%2Ficons%2Ffacebook%2F000%2F029%2F405%2Fjordan.jpg)\n",
    "\n",
    ":P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
