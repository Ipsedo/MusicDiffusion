{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDPM improvements\n",
    "\n",
    "## Sortie de l'espace de définition\n",
    "\n",
    "Problème notoire : les données générées ne sont pas contraintes de rester dans l'intervalle de définition d'un pixel, la preuve :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from music_diffusion.networks import Denoiser, Noiser, normal_kl_div\n",
    "from music_diffusion.data import OUTPUT_SIZES\n",
    "import torch as th\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_channels = 2\n",
    "T = 100\n",
    "denoiser = Denoiser(input_channels, T, 16, 1e-4, 2e-2, [(8, 16), (16, 24), (24, 32)], 8)\n",
    "\n",
    "betas = th.linspace(1e-4, 2e-2, steps=T)\n",
    "alphas = 1 - betas\n",
    "alphas_cum_prod = th.cumprod(alphas, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    denoiser.cuda()\n",
    "    denoiser.eval()\n",
    "\n",
    "    # on tire une image aléatoire : input pour générer la donnée\n",
    "    x_t = th.randn(1, input_channels, *OUTPUT_SIZES, device=\"cuda\")\n",
    "\n",
    "    # les étapes de T - 1 à 0\n",
    "    for t in tqdm(reversed(range(T))):\n",
    "        z = th.randn_like(x_t, device=\"cuda\") if t > 0 else th.zeros_like(x_t, device=\"cuda\")\n",
    "\n",
    "        # predire le bruit\n",
    "        eps_theta, _ = denoiser(\n",
    "            x_t.unsqueeze(1),\n",
    "            th.tensor([[t]], device=\"cuda\"),\n",
    "        )\n",
    "        eps_theta = eps_theta.squeeze(1)\n",
    "\n",
    "        # moyenne\n",
    "        mu = (x_t - eps_theta * (1 - alphas[t]) / th.sqrt(1 - alphas_cum_prod[t])) / th.sqrt(alphas[t])\n",
    "\n",
    "        # variance\n",
    "        var = betas[t]\n",
    "\n",
    "        # création du x à l'étape t - 1\n",
    "        x_t = mu + th.sqrt(var) * z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_t.max(), x_t.min()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solution : en fouillant internet je suis tombé sur cette issue : https://github.com/hojonathanho/diffusion/issues/5\n",
    "\n",
    "Originellement la formule de génération présentée par les auteurs est :\n",
    "\n",
    "![](https://user-images.githubusercontent.com/32613612/118956366-344b0080-b968-11eb-8872-61f7aded9db1.png)\n",
    "\n",
    "Dans la pratique ils n'ont pas codé ça, mais plutôt ceci :\n",
    "\n",
    "![](https://user-images.githubusercontent.com/32613612/118959112-ae7c8480-b96a-11eb-971c-930a6271f7e2.png)\n",
    "\n",
    "Ce qui donne en code :\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alphas_cum_prod_prev = th.cat([th.tensor([1]), alphas_cum_prod[:-1]], dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    denoiser.cuda()\n",
    "    denoiser.eval()\n",
    "\n",
    "    # on tire une image aléatoire : input pour générer la donnée\n",
    "    x_t = th.randn(1, input_channels, *OUTPUT_SIZES, device=\"cuda\")\n",
    "\n",
    "    # les étapes de T - 1 à 0\n",
    "    for t in tqdm(reversed(range(T))):\n",
    "        z = th.randn_like(x_t, device=\"cuda\") if t > 0 else th.zeros_like(x_t, device=\"cuda\")\n",
    "\n",
    "        # predire le bruit\n",
    "        eps_theta, _ = denoiser(\n",
    "            x_t.unsqueeze(1),\n",
    "            th.tensor([[t]], device=\"cuda\"),\n",
    "        )\n",
    "        eps_theta = eps_theta.squeeze(1)\n",
    "\n",
    "        # moyenne\n",
    "        mu_recon = th.clip(x_t / th.sqrt(alphas_cum_prod[t]) - th.sqrt((1 - alphas_cum_prod[t]) / alphas_cum_prod[t]) * eps_theta, -1, 1)\n",
    "        mu = betas[t] * th.sqrt(alphas_cum_prod_prev[t]) / (1 - alphas_cum_prod[t]) * mu_recon + x_t * (1 - alphas_cum_prod_prev[t]) * th.sqrt(alphas[t]) / (1 - alphas_cum_prod[t])\n",
    "\n",
    "        # variance\n",
    "        var = betas[t]\n",
    "\n",
    "        # création du x à l'étape t - 1\n",
    "        x_t = mu + th.sqrt(var) * z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Et nous restons bien dans l'intervalle [-1;1]!\n",
    "\n",
    "L'explication de pourquoi c'est toujours juste théoriquement ? Aucune idée lol."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_t.min(), x_t.max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "D'après la issue GitHub, il s'agit d'un \"hack\" mais qui fonctionne !\n",
    "\n",
    "## Autre amélioration : variance interpolation\n",
    "\n",
    "Une autre amélioration (disponible sur le papier DDPM improvements) est de ne plus mettre une constante pour la variance à chaque étape, mais de laisser le choix à l'algorithme d'utiliser entre deux facteurs temporels (facteurs qui dépendent de l'étape de diffusion) :\n",
    "\n",
    "![](./resources/v_interpolation.png)\n",
    "\n",
    "avec \"betas tiddle\" :\n",
    "\n",
    "![](./resources/beta_tiddle.png)\n",
    "\n",
    "OK donc il faut rajouter une autre sortie pour note UNet, sortie qui devra être entre [0; 1] pour l'interpolation.\n",
    "\n",
    "Le tout avec une sigmoid ! Les auteurs ne contraignent pas l'espace de sortie (couche sans activation), dans la pratique ça se transforme en NaN :,(\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(denoiser)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Une étape de la loop de génération sera la suivante (seulement la partie pour la variance) :\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "betas_tiddle = betas * (1 - alphas_cum_prod_prev) / (1 - alphas_cum_prod)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    t = 99\n",
    "\n",
    "    denoiser.cpu()\n",
    "    x_t = th.randn_like(x_t, device=\"cpu\")\n",
    "    z = th.randn_like(x_t, device=\"cpu\")\n",
    "\n",
    "    eps_theta, v_theta = denoiser(\n",
    "        x_t.unsqueeze(1),\n",
    "        th.tensor([[t]]),\n",
    "    )\n",
    "    eps_theta = eps_theta.squeeze(1)\n",
    "    v_theta = v_theta.squeeze(1)\n",
    "\n",
    "    # variance\n",
    "    var = th.exp(v_theta * th.log(betas[t]) + (1 - v_theta) * th.log(betas_tiddle[t]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "var.max(), var.min()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La génération déjà codée par mes soins :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    x_0 = denoiser.sample(th.randn(1, input_channels, *OUTPUT_SIZES), verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_0.min(), x_0.max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok mais comment on l'intègre dans la MSE du bruit versus le bruit prédit ?\n",
    "\n",
    "=> Avec un nouvel objectif : la divergence de Kullback-Leibler (à additionner avec la MSE) :\n",
    "\n",
    "$$KL(q(x_{t-1}|x_{t}, x_{0}) \\| p_{\\theta}(x_{t-1}|x_{t}))$$\n",
    "\n",
    "Ok mais comment on calcule cette divergence de KL ?\n",
    "\n",
    "=> Il faut calculer la moyenne et la variance pour nos deux distributions : celle du \"noiser\" et celle du \"denoiser\". De la même manière qu'on calculait la moyenne et la variance lors de la génération pour le denoiser, pour le noiser c'est dans l'article.\n",
    "\n",
    "La divergence de KL se fera donc entre ces deux paires de moyennes et de variances :\n",
    "\n",
    "![](./resources/normal_kl_div.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Last pb\n",
    "\n",
    "Ok c'est cool ces améliorations mais est-ce que ça change la donne ? Oui et non :\n",
    "La donnée est bien dans l'espace de définition mais :\n",
    "- la divergence de KL **explose** (milliard de milliards)\n",
    "- Le modèle \"boude\" la magnitude (presque **aucun motifs musicaux**, du silence en gros)\n",
    "\n",
    "Solution :\n",
    "- sommer pour la divergence de KL sur les pixels du spectrogramme\n",
    "- ajouter un facteur pour la divergence de KL (on divise par le max selon le batch) pour éviter qu'elle n'explose\n",
    "- enlever la MSE qui \"interfère\" avec la divergence de KL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noiser = Noiser(T, 1e-4, 2e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final train loop body\n",
    "\n",
    "batch_size = 1\n",
    "step_batch_size = 2\n",
    "\n",
    "t = th.randint(0, T, (batch_size, step_batch_size,))\n",
    "\n",
    "x_t, _ = noiser(x_0, t)\n",
    "eps_theta, v_theta = denoiser(x_t, t)\n",
    "\n",
    "# Avant\n",
    "# loss = mse(eps, eps_theta)\n",
    "# loss = loss.mean()\n",
    "\n",
    "# Maintenant\n",
    "q_mu, q_var = noiser.posterior(x_t, x_0, t)\n",
    "p_mu, p_var = denoiser.prior(x_t, t, eps_theta, v_theta)\n",
    "\n",
    "print(q_mu.size(), q_var.size())\n",
    "print(p_mu.size(), p_var.size())\n",
    "\n",
    "loss = normal_kl_div(q_mu, q_var, p_mu, p_var)\n",
    "print(loss.size(), loss.max())\n",
    "\n",
    "loss = loss / loss.detach().max()\n",
    "print(loss.size(), loss.max())\n",
    "\n",
    "loss = loss.mean()\n",
    "print(loss.size(), loss.max())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's go générer de la musique !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
